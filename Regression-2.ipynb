{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "61b71c93-b625-4fb9-a7fc-ce7089f99c6a",
   "metadata": {},
   "source": [
    "### Q1. R-squared in Linear Regression\n",
    "\n",
    "**Concept:** R-squared (R²) is a statistical measure that represents the proportion of the variance in the dependent variable that is predictable from the independent variables in a linear regression model.\n",
    "\n",
    "**Calculation:** It is calculated as:\n",
    "\n",
    "\\[ R^2 = 1 - \\frac{\\text{SS}_{\\text{res}}}{\\text{SS}_{\\text{tot}}} \\]\n",
    "\n",
    "where:\n",
    "- \\(\\text{SS}_{\\text{res}}\\) is the sum of squares of residuals (errors), i.e., \\(\\sum (y_i - \\hat{y}_i)^2\\),\n",
    "- \\(\\text{SS}_{\\text{tot}}\\) is the total sum of squares, i.e., \\(\\sum (y_i - \\bar{y})^2\\).\n",
    "\n",
    "**Representation:** R² represents the proportion of the total variation in the dependent variable that is explained by the independent variables. An R² of 1 indicates that the model explains all the variability in the response variable, while an R² of 0 indicates that the model explains none of the variability."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7803ac4b-e149-4e64-a4e7-10b3055497b6",
   "metadata": {},
   "source": [
    "### Q2. Adjusted R-squared\n",
    "\n",
    "**Definition:** Adjusted R-squared adjusts the R² value for the number of predictors in the model. It accounts for the fact that adding more predictors will always increase R², even if those predictors are not meaningful.\n",
    "\n",
    "**Formula:**\n",
    "\n",
    "\\[ \\text{Adjusted } R^2 = 1 - \\left(\\frac{1 - R^2}{n - p - 1}\\right) \\times (n - 1) \\]\n",
    "\n",
    "where \\(n\\) is the number of observations and \\(p\\) is the number of predictors.\n",
    "\n",
    "**Difference from R-squared:** Unlike R², adjusted R² can decrease if the added predictors do not improve the model sufficiently. It provides a more accurate measure of model fit when comparing models with different numbers of predictors."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9c27e79-6886-41fd-bac1-52040a9ed611",
   "metadata": {},
   "source": [
    "### Q3. When to Use Adjusted R-squared\n",
    "\n",
    "Adjusted R² is more appropriate when comparing models with different numbers of predictors. It helps in understanding whether the inclusion of additional predictors genuinely improves the model or if they just inflate the R² value."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7a7f55c-f23b-4545-b24c-43588235ed84",
   "metadata": {},
   "source": [
    "### Q4. RMSE, MSE, and MAE\n",
    "\n",
    "**RMSE (Root Mean Squared Error):** Measures the square root of the average of the squared errors. It gives higher weight to larger errors.\n",
    "\n",
    "\\[ \\text{RMSE} = \\sqrt{\\frac{1}{n}\\sum_{i=1}^n (y_i - \\hat{y}_i)^2} \\]\n",
    "\n",
    "**MSE (Mean Squared Error):** Measures the average of the squared errors.\n",
    "\n",
    "\\[ \\text{MSE} = \\frac{1}{n}\\sum_{i=1}^n (y_i - \\hat{y}_i)^2 \\]\n",
    "\n",
    "**MAE (Mean Absolute Error):** Measures the average of the absolute errors. It provides a straightforward measure of prediction accuracy.\n",
    "\n",
    "\\[ \\text{MAE} = \\frac{1}{n}\\sum_{i=1}^n |y_i - \\hat{y}_i| \\]\n",
    "\n",
    "**Representation:** These metrics assess the average size of the prediction errors, with RMSE and MSE being more sensitive to larger errors due to squaring, while MAE provides a more direct measure of average error."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8d3ea0f-06e3-441c-8ebd-9d69421bf2c7",
   "metadata": {},
   "source": [
    "### Q5. Advantages and Disadvantages of RMSE, MSE, and MAE\n",
    "\n",
    "**Advantages:**\n",
    "- **RMSE:** Sensitive to large errors, which can be useful if large errors are particularly undesirable.\n",
    "- **MSE:** Simple to compute and interpret. Like RMSE, it penalizes larger errors more heavily.\n",
    "- **MAE:** Provides a clear, interpretable metric of average prediction error. It is robust to outliers compared to RMSE.\n",
    "\n",
    "**Disadvantages:**\n",
    "- **RMSE:** Can be overly sensitive to outliers due to squaring of errors.\n",
    "- **MSE:** Also sensitive to outliers and may not be as interpretable as MAE.\n",
    "- **MAE:** Does not penalize large errors as much as RMSE or MSE, which might be important in some applications."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69c2e75c-48ff-41f7-8996-8fefa1b881c9",
   "metadata": {},
   "source": [
    "### Q6. Lasso vs. Ridge Regularization\n",
    "\n",
    "**Lasso Regularization (L1):** Adds a penalty equal to the absolute value of the magnitude of coefficients. This can lead to some coefficients being exactly zero, effectively performing feature selection.\n",
    "\n",
    "**Ridge Regularization (L2):** Adds a penalty equal to the square of the magnitude of coefficients. It tends to shrink coefficients but does not force them to zero, hence does not perform feature selection.\n",
    "\n",
    "**When to Use:**\n",
    "- **Lasso:** When you want to perform feature selection and potentially reduce the number of features.\n",
    "- **Ridge:** When you want to handle multicollinearity and reduce model complexity without eliminating features."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5ac768c-7ba2-4b8d-a263-92dc42cb5295",
   "metadata": {},
   "source": [
    "### Q7. Regularized Linear Models and Overfitting\n",
    "\n",
    "Regularized linear models help prevent overfitting by adding a penalty to the size of the coefficients, which discourages overly complex models. For example, Ridge regularization will shrink large coefficients, while Lasso can remove them entirely.\n",
    "\n",
    "**Example:** If a linear regression model with many predictors is overfitting the training data, applying Lasso regularization might result in a simpler model with fewer predictors, improving generalization to new data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a710ce84-73d2-4ca9-bb01-03f5f2d5d96c",
   "metadata": {},
   "source": [
    "### Q8. Limitations of Regularized Linear Models\n",
    "\n",
    "**Limitations:**\n",
    "- **Assumption of Linearity:** Regularized linear models assume a linear relationship between predictors and the response variable. They may not perform well with non-linear relationships.\n",
    "- **Feature Selection (Lasso):** While Lasso can perform feature selection, it may sometimes exclude useful predictors if the regularization strength is too high.\n",
    "- **Interpretability:** Regularization can make interpretation more complex, particularly when many predictors are involved.\n",
    "\n",
    "### Q9. Choosing Between Models Based on RMSE and MAE\n",
    "\n",
    "**Model A (RMSE = 10) vs. Model B (MAE = 8):** \n",
    "\n",
    "- **Choosing Model B**: If you prioritize minimizing the average error, MAE might be more relevant. However, RMSE gives more weight to large errors, which could be more important depending on your application.\n",
    "- **Limitations:** RMSE's sensitivity to large errors might make it a better choice in some contexts, even if it results in higher average error (as measured by MAE).\n",
    "\n",
    "### Q10. Comparing Regularized Models\n",
    "\n",
    "**Model A (Ridge, λ = 0.1) vs. Model B (Lasso, λ = 0.5):**\n",
    "\n",
    "- **Model Choice:** The choice depends on your goals. Ridge regularization with a lower λ might be more suitable if you want to keep all features but with smaller coefficients. Lasso with a higher λ might be better if you aim for feature selection and a sparser model.\n",
    "- **Trade-offs:** Lasso can be more interpretable due to feature selection but might miss important predictors if λ is too high. Ridge generally maintains all predictors but doesn’t perform feature selection.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15b471fa-357d-4ce8-a9cf-06f2887f00a6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
