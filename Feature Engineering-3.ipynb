{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1b98412d-8373-4cb8-827f-084c6dc061c1",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "**Q1. What is Min-Max scaling, and how is it used in data preprocessing? Provide an example to illustrate its application.**\n",
    "\n",
    "Min-Max scaling (also known as normalization) transforms the features to a fixed range, typically between 0 and 1. It is calculated using the formula:\n",
    "\\[ X_{\\text{scaled}} = \\frac{X - X_{\\text{min}}}{X_{\\text{max}} - X_{\\text{min}}} \\]\n",
    "where \\( X_{\\text{min}} \\) and \\( X_{\\text{max}} \\) are the minimum and maximum values of the feature \\( X \\), respectively.\n",
    "\n",
    "**Example:**\n",
    "Suppose we have a dataset of prices ranging from $10 to $100. Applying Min-Max scaling:\n",
    "- Original prices: [10, 20, 30, 40, 50, 60, 70, 80, 90, 100]\n",
    "- After Min-Max scaling: [0.0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 1.0]\n",
    "\n",
    "**Q2. What is the Unit Vector technique in feature scaling, and how does it differ from Min-Max scaling? Provide an example to illustrate its application.**\n",
    "\n",
    "The Unit Vector technique (or Unit Norm) scales each feature so that the vector of the feature values has unit norm. It can be expressed as:\n",
    "\\[ X_{\\text{scaled}} = \\frac{X}{\\|X\\|} \\]\n",
    "where \\( \\|X\\| \\) is the Euclidean norm of \\( X \\).\n",
    "\n",
    "**Example:**\n",
    "Consider a dataset where the feature vector is [3, 4]. The Euclidean norm is \\( \\sqrt{3^2 + 4^2} = 5 \\).\n",
    "- After Unit Vector scaling: [0.6, 0.8]\n",
    "\n",
    "Unit Vector scaling normalizes each feature vector to have unit length.\n",
    "\n",
    "**Q3. What is PCA (Principal Component Analysis), and how is it used in dimensionality reduction? Provide an example to illustrate its application.**\n",
    "\n",
    "PCA is a technique used to reduce the dimensionality of a dataset while preserving as much variance as possible. It does this by identifying a new set of orthogonal axes (principal components) that capture the directions of maximum variance in the data.\n",
    "\n",
    "**Example:**\n",
    "Suppose we have a dataset with multiple features (dimensions). PCA will transform this dataset into a new set of principal components, ordered by the amount of variance they explain. This reduces the dimensionality of the dataset while retaining the most important information.\n",
    "\n",
    "**Q4. What is the relationship between PCA and Feature Extraction, and how can PCA be used for Feature Extraction? Provide an example to illustrate this concept.**\n",
    "\n",
    "PCA can be used for feature extraction by transforming the original features into a smaller set of principal components. These components are linear combinations of the original features and represent the directions of maximum variance in the data.\n",
    "\n",
    "**Example:**\n",
    "In a dataset with 10 features, PCA might identify that the first 3 principal components explain 95% of the variance. Instead of using all 10 features, you can use these 3 principal components for further analysis or modeling, effectively reducing the dimensionality and complexity of the data.\n",
    "\n",
    "**Q5. You are working on a project to build a recommendation system for a food delivery service. The dataset contains features such as price, rating, and delivery time. Explain how you would use Min-Max scaling to preprocess the data.**\n",
    "\n",
    "In this case, you would apply Min-Max scaling to normalize the ranges of the features like price, rating, and delivery time to a common scale, typically between 0 and 1. This ensures that all features contribute equally to the analysis without one feature dominating due to its larger numerical range.\n",
    "\n",
    "**Q6. You are working on a project to build a model to predict stock prices. The dataset contains many features, such as company financial data and market trends. Explain how you would use PCA to reduce the dimensionality of the dataset.**\n",
    "\n",
    "PCA would help reduce the dimensionality by transforming the original features (financial data and market trends) into a smaller set of principal components. These components would capture the most important patterns and variance in the data, allowing for simpler and potentially more effective models.\n",
    "\n",
    "**Q7. For a dataset containing the following values: [1, 5, 10, 15, 20], perform Min-Max scaling to transform the values to a range of -1 to 1.**\n",
    "\n",
    "To perform Min-Max scaling to the range -1 to 1:\n",
    "- \\( X_{\\text{scaled}} = \\frac{X - X_{\\text{min}}}{X_{\\text{max}} - X_{\\text{min}}} \\times (2) - 1 \\)\n",
    "\n",
    "Calculating for the dataset:\n",
    "- \\( X_{\\text{min}} = 1 \\)\n",
    "- \\( X_{\\text{max}} = 20 \\)\n",
    "\n",
    "Scaled values:\n",
    "\\[ \\text{Scaled} = \\left[ \\frac{1 - 1}{20 - 1} \\times 2 - 1, \\frac{5 - 1}{20 - 1} \\times 2 - 1, \\ldots, \\frac{20 - 1}{20 - 1} \\times 2 - 1 \\right] \\]\n",
    "\\[ \\text{Scaled} = \\left[ -1, -0.5, 0, 0.5, 1 \\right] \\]\n",
    "\n",
    "**Q8. For a dataset containing the following features: [height, weight, age, gender, blood pressure], perform Feature Extraction using PCA. How many principal components would you choose to retain, and why?**\n",
    "\n",
    "To determine how many principal components to retain, you would typically look at the explained variance ratio. This ratio tells you how much variance each principal component explains. A common approach is to choose the number of principal components that together explain a significant portion (e.g., 95%) of the total variance in the dataset.\n",
    "\n",
    "After applying PCA to your dataset, you would examine the cumulative explained variance and select the appropriate number of principal components that retain most of the information while reducing dimensionality. The number of principal components retained would depend on the specific dataset and the desired balance between dimensionality reduction and information preservation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fff9221d-b81d-4e10-a9f8-56f96cbd8f00",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
