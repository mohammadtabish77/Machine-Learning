{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bfa984c9-a9e3-4621-a50a-62e54dbf14df",
   "metadata": {},
   "source": [
    "### Q1. Relationship Between Polynomial Functions and Kernel Functions\n",
    "\n",
    "In machine learning algorithms, particularly in Support Vector Machines (SVMs), polynomial functions and kernel functions are closely related:\n",
    "\n",
    "- **Polynomial Functions**: These are mathematical functions of the form \\( P(x) = a_n x^n + a_{n-1} x^{n-1} + \\cdots + a_1 x + a_0 \\). They can capture interactions between features up to the \\(n\\)-th degree.\n",
    "\n",
    "- **Kernel Functions**: In SVM, kernel functions allow the algorithm to operate in a higher-dimensional space without explicitly transforming the data. This is useful for capturing complex relationships in the data. The polynomial kernel function is one such kernel, defined as:\n",
    "  \\[ K(\\mathbf{x}, \\mathbf{x}') = (\\mathbf{x}^\\top \\mathbf{x}' + c)^d \\]\n",
    "  Here:\n",
    "  - \\( \\mathbf{x} \\) and \\( \\mathbf{x}' \\) are input feature vectors.\n",
    "  - \\( c \\) is a constant that adjusts the kernel function.\n",
    "  - \\( d \\) is the degree of the polynomial.\n",
    "\n",
    "  The polynomial kernel enables SVMs to learn polynomial decision boundaries, effectively mapping the data into a higher-dimensional space where linear separation is possible.\n",
    "\n",
    "### Q2. Implementing an SVM with a Polynomial Kernel in Python using Scikit-learn\n",
    "\n",
    "Here’s how you can implement an SVM with a polynomial kernel in Python using Scikit-learn:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8795c31e-3441-428c-964c-6b9c3becaef6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.96\n"
     ]
    }
   ],
   "source": [
    "from sklearn import datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Load dataset\n",
    "iris = datasets.load_iris()\n",
    "X = iris.data\n",
    "y = iris.target\n",
    "\n",
    "# Split the dataset\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Preprocess data\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "# Create and train SVM with polynomial kernel\n",
    "clf = SVC(kernel='poly', degree=3, C=1.0)\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "# Predict and evaluate\n",
    "y_pred = clf.predict(X_test)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Accuracy: {accuracy:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b02da2a2-0ede-4a58-98b1-b2aa69bcf25d",
   "metadata": {},
   "source": [
    "### Q3. Effect of Increasing the Value of Epsilon on Support Vectors in SVR\n",
    "\n",
    "In Support Vector Regression (SVR), epsilon (\\(\\epsilon\\)) specifies the margin of tolerance where no penalty is given for errors within this margin. \n",
    "\n",
    "- **Increasing \\(\\epsilon\\)**: Increases the width of the tube within which errors are ignored. This can lead to fewer support vectors because more predictions fall within the tube and are not penalized.\n",
    "\n",
    "- **Decreasing \\(\\epsilon\\)**: Narrows the tube, potentially increasing the number of support vectors because more predictions fall outside the tube and are penalized.\n",
    "\n",
    "### Q4. Effect of Hyperparameters on SVR Performance\n",
    "\n",
    "Here’s how different parameters affect the performance of Support Vector Regression (SVR):\n",
    "\n",
    "- **Kernel Function**: Determines the transformation applied to the input data. Common choices are:\n",
    "  - **Linear**: For linearly separable data.\n",
    "  - **Polynomial**: Captures interactions between features.\n",
    "  - **RBF (Radial Basis Function)**: Effective for non-linear data.\n",
    "  - **Example**: For complex non-linear relationships, the RBF kernel might perform better.\n",
    "\n",
    "- **C Parameter**: Controls the trade-off between achieving a low error on the training data and minimizing the model complexity.\n",
    "  - **High C**: Less tolerance for errors, may lead to overfitting.\n",
    "  - **Low C**: More tolerance for errors, may lead to underfitting.\n",
    "  - **Example**: Increase \\(C\\) for more precise fitting, decrease for smoother models.\n",
    "\n",
    "- **Epsilon Parameter (\\(\\epsilon\\))**: Defines the margin of tolerance where no penalty is given for errors. \n",
    "  - **High \\(\\epsilon\\)**: More tolerance for errors, fewer support vectors.\n",
    "  - **Low \\(\\epsilon\\)**: Less tolerance, more support vectors.\n",
    "  - **Example**: Increase \\(\\epsilon\\) to capture more general trends, decrease for fine-grained fitting.\n",
    "\n",
    "- **Gamma Parameter (\\(\\gamma\\))**: Defines the influence of a single training example.\n",
    "  - **High \\(\\gamma\\)**: Models closer to the training data, which can lead to overfitting.\n",
    "  - **Low \\(\\gamma\\)**: Models broader regions of space, which can lead to underfitting.\n",
    "  - **Example**: Increase \\(\\gamma\\) for more localized models, decrease for broader, more general models.\n",
    "\n",
    "### Q5. Assignment Implementation\n",
    "\n",
    "Let’s go through the steps of the assignment:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9ce9df65-ef7e-4c2c-b88e-df7d42524726",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial Accuracy: 1.00\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00        19\n",
      "           1       1.00      1.00      1.00        13\n",
      "           2       1.00      1.00      1.00        13\n",
      "\n",
      "    accuracy                           1.00        45\n",
      "   macro avg       1.00      1.00      1.00        45\n",
      "weighted avg       1.00      1.00      1.00        45\n",
      "\n",
      "Best Parameters: {'C': 10, 'degree': 2, 'kernel': 'linear'}\n",
      "Model saved as 'svc_model.pkl'\n"
     ]
    }
   ],
   "source": [
    "from sklearn import datasets\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "import joblib\n",
    "\n",
    "# Load dataset\n",
    "dataset = datasets.load_iris()\n",
    "X = dataset.data\n",
    "y = dataset.target\n",
    "\n",
    "# Split the dataset\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Preprocess data\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "# Create and train SVC classifier\n",
    "svc = SVC()\n",
    "svc.fit(X_train, y_train)\n",
    "\n",
    "# Predict and evaluate\n",
    "y_pred = svc.predict(X_test)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Initial Accuracy: {accuracy:.2f}\")\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "# Hyperparameter tuning with GridSearchCV\n",
    "param_grid = {\n",
    "    'C': [0.1, 1, 10],\n",
    "    'kernel': ['linear', 'poly', 'rbf'],\n",
    "    'degree': [2, 3]  # Only relevant for polynomial kernel\n",
    "}\n",
    "grid_search = GridSearchCV(SVC(), param_grid, cv=5)\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "print(f\"Best Parameters: {grid_search.best_params_}\")\n",
    "\n",
    "# Train the tuned classifier on the entire dataset\n",
    "best_svc = grid_search.best_estimator_\n",
    "best_svc.fit(X, y)\n",
    "\n",
    "# Save the trained classifier\n",
    "joblib.dump(best_svc, 'svc_model.pkl')\n",
    "\n",
    "print(\"Model saved as 'svc_model.pkl'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bf4da55-e23c-41a2-893f-05653b0bd11e",
   "metadata": {},
   "source": [
    "### Summary\n",
    "\n",
    "1. **Polynomial Functions and Kernels**: Polynomial kernels allow SVMs to learn polynomial decision boundaries by implicitly mapping data to higher dimensions.\n",
    "2. **Implementing Polynomial Kernel**: Use `SVC(kernel='poly')` with Scikit-learn to create an SVM with a polynomial kernel.\n",
    "3. **Epsilon in SVR**: Increasing \\(\\epsilon\\) generally reduces the number of support vectors.\n",
    "4. **SVR Parameters**: Each parameter affects model complexity, generalization, and fitting precision. Tuning them based on the data is essential.\n",
    "5. **Assignment**: Load data, preprocess, train and evaluate SVM, tune hyperparameters, and save the model.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fe079a5-c072-4ca1-9145-ea0894531dda",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
