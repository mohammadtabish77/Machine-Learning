{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c7c52ebb-8f2a-491d-a8f2-56ab0cbf785b",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "### Q1. What is Bayes' Theorem?\n",
    "\n",
    "**Bayes' Theorem** is a principle in probability theory that describes how to update the probability of a hypothesis based on new evidence. It provides a way to calculate conditional probabilities, which is fundamental in many statistical inference problems.\n",
    "\n",
    "### Q2. What is the Formula for Bayes' Theorem?\n",
    "\n",
    "The formula for Bayes' Theorem is:\n",
    "\n",
    "\\[ P(C_i | X) = \\frac{P(X | C_i) \\cdot P(C_i)}{P(X)} \\]\n",
    "\n",
    "where:\n",
    "- \\( P(C_i | X) \\) is the posterior probability of class \\( C_i \\) given the features \\( X \\).\n",
    "- \\( P(X | C_i) \\) is the likelihood of features \\( X \\) given the class \\( C_i \\).\n",
    "- \\( P(C_i) \\) is the prior probability of class \\( C_i \\).\n",
    "- \\( P(X) \\) is the marginal probability of features \\( X \\).\n",
    "\n",
    "### Q3. How is Bayes' Theorem Used in Practice?\n",
    "\n",
    "In practice, Bayes' Theorem is used to:\n",
    "- **Classify data**: In machine learning, it's used to classify instances based on the likelihood of features given the classes.\n",
    "- **Update beliefs**: In Bayesian statistics, it's used to update the probability of a hypothesis as more evidence or information becomes available.\n",
    "- **Decision making**: It helps in decision-making processes where probabilistic models are required.\n",
    "\n",
    "### Q4. What is the Relationship Between Bayes' Theorem and Conditional Probability?\n",
    "\n",
    "Bayes' Theorem is directly related to conditional probability. It calculates the conditional probability of an event \\( C_i \\) (class) given another event \\( X \\) (features) by using the reverse conditional probability \\( P(X | C_i) \\). Essentially, it helps in updating the probability of a class based on observed features.\n",
    "\n",
    "### Q5. How Do You Choose Which Type of Naive Bayes Classifier to Use for Any Given Problem?\n",
    "\n",
    "Naive Bayes classifiers come in several types, each suited to different kinds of data:\n",
    "- **Gaussian Naive Bayes**: Assumes that the features follow a Gaussian (normal) distribution. Useful when the features are continuous and normally distributed.\n",
    "- **Multinomial Naive Bayes**: Assumes that the features follow a multinomial distribution. It is commonly used for text classification tasks where features are word counts or frequencies.\n",
    "- **Bernoulli Naive Bayes**: Assumes that features are binary (0 or 1). It is used when features are binary or represent the presence/absence of certain attributes.\n",
    "\n",
    "**Choosing the Type**:\n",
    "- Use **Gaussian Naive Bayes** for continuous data that follows a normal distribution.\n",
    "- Use **Multinomial Naive Bayes** for categorical features or text data with word counts or frequencies.\n",
    "- Use **Bernoulli Naive Bayes** for binary features or data representing the presence/absence of features.\n",
    "\n",
    "### Q6. Assignment: Predicting Class for New Instance\n",
    "\n",
    "You have a dataset with the following frequency table:\n",
    "\n",
    "| Class | X1=1 | X1=2 | X1=3 | X2=1 | X2=2 | X2=3 | X2=4 |\n",
    "|-------|------|------|------|------|------|------|------|\n",
    "| A     | 3    | 3    | 4    | 4    | 3    | 3    | 3    |\n",
    "| B     | 2    | 2    | 1    | 2    | 2    | 2    | 3    |\n",
    "\n",
    "You want to classify a new instance with \\( X1 = 3 \\) and \\( X2 = 4 \\).\n",
    "\n",
    "#### 1. Calculate the Prior Probabilities\n",
    "\n",
    "Assuming equal prior probabilities for each class:\n",
    "\n",
    "\\[ P(A) = P(B) = \\frac{1}{2} \\]\n",
    "\n",
    "#### 2. Calculate the Likelihoods\n",
    "\n",
    "**For Class A:**\n",
    "\n",
    "- Number of instances where \\( X1 = 3 \\) and \\( X2 = 4 \\):\n",
    "  - \\( P(X1 = 3 | A) = \\frac{4}{3+3+4} = \\frac{4}{10} \\)\n",
    "  - \\( P(X2 = 4 | A) = \\frac{3}{4+3+3+3} = \\frac{3}{13} \\)\n",
    "\n",
    "**For Class B:**\n",
    "\n",
    "- Number of instances where \\( X1 = 3 \\) and \\( X2 = 4 \\):\n",
    "  - \\( P(X1 = 3 | B) = \\frac{1}{2+2+1} = \\frac{1}{5} \\)\n",
    "  - \\( P(X2 = 4 | B) = \\frac{3}{2+2+2+3} = \\frac{3}{9} \\)\n",
    "\n",
    "#### 3. Apply Bayes' Theorem\n",
    "\n",
    "**For Class A:**\n",
    "\n",
    "\\[ P(A | X1=3, X2=4) \\propto P(X1=3 | A) \\cdot P(X2=4 | A) \\cdot P(A) \\]\n",
    "\n",
    "\\[ P(A | X1=3, X2=4) \\propto \\frac{4}{10} \\cdot \\frac{3}{13} \\cdot \\frac{1}{2} \\]\n",
    "\n",
    "**For Class B:**\n",
    "\n",
    "\\[ P(B | X1=3, X2=4) \\propto P(X1=3 | B) \\cdot P(X2=4 | B) \\cdot P(B) \\]\n",
    "\n",
    "\\[ P(B | X1=3, X2=4) \\propto \\frac{1}{5} \\cdot \\frac{3}{9} \\cdot \\frac{1}{2} \\]\n",
    "\n",
    "#### 4. Compare the Results\n",
    "\n",
    "- **For Class A:**\n",
    "\n",
    "\\[ P(A | X1=3, X2=4) \\propto \\frac{4}{10} \\cdot \\frac{3}{13} \\cdot \\frac{1}{2} \\approx 0.01846 \\]\n",
    "\n",
    "- **For Class B:**\n",
    "\n",
    "\\[ P(B | X1=3, X2=4) \\propto \\frac{1}{5} \\cdot \\frac{3}{9} \\cdot \\frac{1}{2} \\approx 0.01667 \\]\n",
    "\n",
    "**Prediction**: The posterior probability for Class A is higher than for Class B, so Naive Bayes would predict that the new instance belongs to **Class A**.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b94ebe3-635f-4ba1-9255-923296a9e67b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
